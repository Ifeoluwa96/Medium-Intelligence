{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "data.reverse()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c2842511c116>, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c2842511c116>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    out = [self.neuron_value[neuron_string][enum]+=self.cost_function(l,\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Network_classify:\n",
    "    def __init__(self, layers, data, target):\n",
    "        #layers have to be numbers of neurons for each layer\n",
    "        self.no_of_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.eval_fit = []\n",
    "        var = 0\n",
    "        \n",
    "        \n",
    "        #pre-initialize weights\n",
    "        self.weights = {}\n",
    "        for k in layers:\n",
    "            self.weights['weight'+str(k)] = np.random.randn(int(k))\n",
    "            \n",
    "            \n",
    "        #pre-intialize biases\n",
    "        for k in layers:\n",
    "            self.biases['bias'+str(k)] = np.random.randn(int(k))\n",
    "            \n",
    "        #pre_initialized Neuron_value\n",
    "        for k in layers:\n",
    "            self.neuron_value['neuron'+str(k)] = np.random.randn(int(k))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def cost_function(self, x, weight, bias):\n",
    "        return x*weight + bias\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x_val):\n",
    "        # at the layer\n",
    "        for k in self.layer:\n",
    "            neuron_string = 'neuron'+str(k)\n",
    "            weight_string = 'weight'+ str(k)\n",
    "            bias_string = 'bias'+str(k)\n",
    "            \n",
    "            \n",
    "            #update each neuron value from curent weight and bias\n",
    "            for enum,i in enumerate(self.neuron_value[neuron_string]):\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                serious correction is needed here in updating neuron values\n",
    "                \"\"\"\n",
    "                out = [self.neuron_value[neuron_string][enum]+=self.cost_function(l,\n",
    "                                               self.weights[weight_string][enum],\n",
    "                                               self.bias[bias_string][enum]) for l in x_val]\n",
    "            x_val = self.neuron_value[neuron_string]\n",
    "        return \n",
    "    \n",
    "    def batches(self,x, batch_size = 50):\n",
    "        dict_batch,count = {},0\n",
    "        for enum, i in x:\n",
    "            if enum% 50 == 0 and (x[0]-enum)>batch_size:\n",
    "                start = enum\n",
    "                end = start + batch_size\n",
    "                dict_batch[str(count)] = x[start:stop]\n",
    "                count += 1\n",
    "            elif (x[0]- enum)< batch_size:\n",
    "                start = count*batch_size\n",
    "                dict_batch[str(count)] = x[start:]\n",
    "            else:\n",
    "                pass\n",
    "        return dict_batch\n",
    "    \n",
    "    \n",
    "    def feedforward(self,x, batch_size = 50):\n",
    "        for val_x in x:\n",
    "            out = [self.forward(val_x) for k in val_x]\n",
    "        return\n",
    "    \n",
    "    def update(self, weight_string,bias_string,neuron_string enum,learning_rate):\n",
    "        current_neuron_value = self.neuron_value[neuron_string][enum]\n",
    "        self.weights[weight_string][enum] -= learning_rate* current_neuron_value\n",
    "        self.bias[bias_string][enum] -= learning_rate\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def backprop(self, learning_rate, batch_size= 50):\n",
    "        copy_layer = self.layers\n",
    "        copy_layer.reverse()\n",
    "        for k in copy_layer:\n",
    "            neuron_string = 'neuron'+str(k)\n",
    "            weight_string = 'weight'+ str(k)\n",
    "            bias_string = 'bias'+str(k)\n",
    "            \n",
    "            out = [self.update(weight_string, bias_string,\n",
    "                               neuron_string, enum, \n",
    "                               learning_rate) for enum,i in enumerate(self.neuron_value[neuron_string])]\n",
    "            \n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, data, target,epochs, learning_rate, batch_size = 50, metric =['Accuracy', 'precision','f1_score']):\n",
    "        \"\"\"\n",
    "            Fit in batches and compute for each epoch epochs per batch\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            # feed forward\n",
    "            self.feedforward(data, batch_size = batch_size)\n",
    "            \n",
    "            # backpropagation \n",
    "            self.backprop(learning_rate, batch_size = batch_size)\n",
    "            \n",
    "            # evaluation\n",
    "            score = self.evaluate(data,y, evaluation_type = metric)\n",
    "            self.eval_fit[str(i)] = score\n",
    "        return\n",
    "    def bit_compute(self, x_val):\n",
    "        # at the layer\n",
    "        layer_copy = self.layer.copy()        \n",
    "            \n",
    "        bias, weight,neurons = self.biases, self.weights, self.neuron_value\n",
    "        for k in layer_copy:\n",
    "            neuron_string = 'neuron'+str(k)\n",
    "            weight_string = 'weight'+ str(k)\n",
    "            bias_string = 'bias'+str(k)\n",
    "            \n",
    "            #update each neuron value from curent weight and bias\n",
    "            for enum,i in enumerate(neurons[neuron_string]):\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                serious correction is needed here in updating neuron values\n",
    "                \"\"\"\n",
    "                out = [neurons[neuron_string][enum]+=self.cost_function(l,\n",
    "                                               weight[weight_string][enum],\n",
    "                                               bias[bias_string][enum]) for l in x_val]\n",
    "            x_val = self.neuron_value[neuron_string]\n",
    "        \n",
    "        outp = neurons['neuron'+str(layer_copy[-1])]\n",
    "        return outp\n",
    "            \n",
    "    \n",
    "    def compute(self, data):\n",
    "        output = [self.bit_compute(l) for l in data]\n",
    "        return output\n",
    "    \n",
    "    def evaluate(self, data, y, evaluation_type = ['accuracy','f1_score','precision']):\n",
    "        value = self.compute(data)\n",
    "        evaluation = [int(np.round(i))==int(j) for i,j in zip(value,y)]\n",
    "        \n",
    "        if evaluation_type == ['accuracy']:\n",
    "            out = sum(evaluation)/len(evaluation)\n",
    "        elif 'precision' in evaluation_type:\n",
    "            # precision\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def plot_training(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        #self.eval_fit\n",
    "        x = self.eval_fit.keys()\n",
    "        y = self.eval_fit.values\n",
    "        \n",
    "        sns.lineplot(x,y)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, model = 'All'):\n",
    "        \"\"\"\n",
    "        model can be: bias, weights, neuron_values\n",
    "        save biases\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        # write a file\n",
    "        f = open(\"model.pkl\", \"w\")\n",
    "        #dump biases\n",
    "        pickle.dump(self.biases, f)\n",
    "        #dump weights\n",
    "        pickle.dump(self.weights,f)\n",
    "        #dump neuron values\n",
    "        pickle.dump(self.neuron_value, f)\n",
    "        f.close()\n",
    "        return\n",
    "    \n",
    "    def load_model(self,model_dir =''):\n",
    "        f = open(model_dir, \"r\")\n",
    "        biases = pickle.load(f)\n",
    "        weights = pickle.load(f)\n",
    "        neurons = pickle.load(f)\n",
    "        f.close()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, values):\n",
    "        #\n",
    "        return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_classify:\n",
    "    def __init__(self, layers, data, target):\n",
    "        #layers have to be numbers of neurons for each layer\n",
    "        self.no_of_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.eval_fit = []\n",
    "        var = 0\n",
    "        \n",
    "        \n",
    "        #pre-initialize weights\n",
    "        self.weights = {}\n",
    "        for k in layers:\n",
    "            self.weights['weight'+str(k)] = np.random.randn(int(k))\n",
    "            \n",
    "            \n",
    "        #pre-intialize biases\n",
    "        for k in layers:\n",
    "            self.biases['bias'+str(k)] = np.random.randn(int(k))\n",
    "            \n",
    "        #pre_initialized Neuron_value\n",
    "        for k in layers:\n",
    "            self.neuron_value['neuron'+str(k)] = np.random.randn(int(k))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def cost_function(self, x, weight, bias):\n",
    "        return x*weight + bias\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x_val):\n",
    "        # at the layer\n",
    "        for k in self.layer:\n",
    "            neuron_string = 'neuron'+str(k)\n",
    "            weight_string = 'weight'+ str(k)\n",
    "            bias_string = 'bias'+str(k)\n",
    "            \n",
    "            \n",
    "            #update each neuron value from curent weight and bias\n",
    "            for enum,i in enumerate(self.neuron_value[neuron_string]):\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                serious correction is needed here in updating neuron values\n",
    "                \"\"\"\n",
    "                out = [self.neuron_value[neuron_string][enum]+=self.cost_function(l,\n",
    "                                               self.weights[weight_string][enum],\n",
    "                                               self.bias[bias_string][enum]) for l in x_val]\n",
    "            x_val = self.neuron_value[neuron_string]\n",
    "        return \n",
    "    \n",
    "    def batches(self,x, batch_size = 50):\n",
    "        dict_batch,count = {},0\n",
    "        for enum, i in x:\n",
    "            if enum% 50 == 0 and (x[0]-enum)>batch_size:\n",
    "                start = enum\n",
    "                end = start + batch_size\n",
    "                dict_batch[str(count)] = x[start:stop]\n",
    "                count += 1\n",
    "            elif (x[0]- enum)< batch_size:\n",
    "                start = count*batch_size\n",
    "                dict_batch[str(count)] = x[start:]\n",
    "            else:\n",
    "                pass\n",
    "        return dict_batch\n",
    "    \n",
    "    \n",
    "    def feedforward(self,x, batch_size = 50):\n",
    "        for val_x in x:\n",
    "            out = [self.forward(val_x) for k in val_x]\n",
    "        return\n",
    "    \n",
    "    def update(self, weight_string,bias_string,neuron_string enum,learning_rate):\n",
    "        current_neuron_value = self.neuron_value[neuron_string][enum]\n",
    "        self.weights[weight_string][enum] -= learning_rate* current_neuron_value\n",
    "        self.bias[bias_string][enum] -= learning_rate\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def backprop(self, learning_rate, batch_size= 50):\n",
    "        copy_layer = self.layers\n",
    "        copy_layer.reverse()\n",
    "        for k in copy_layer:\n",
    "            neuron_string = 'neuron'+str(k)\n",
    "            weight_string = 'weight'+ str(k)\n",
    "            bias_string = 'bias'+str(k)\n",
    "            \n",
    "            out = [self.update(weight_string, bias_string,\n",
    "                               neuron_string, enum, \n",
    "                               learning_rate) for enum,i in enumerate(self.neuron_value[neuron_string])]\n",
    "            \n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, data, target,epochs, learning_rate, batch_size = 50, metric =['Accuracy', 'precision','f1_score']):\n",
    "        \"\"\"\n",
    "            Fit in batches and compute for each epoch epochs per batch\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            # feed forward\n",
    "            self.feedforward(data, batch_size = batch_size)\n",
    "            \n",
    "            # backpropagation \n",
    "            self.backprop(learning_rate, batch_size = batch_size)\n",
    "            \n",
    "            # evaluation\n",
    "            score = self.evaluate(data,y, evaluation_type = metric)\n",
    "            self.eval_fit[str(i)] = score\n",
    "        return\n",
    "    def bit_compute(self, x_val):\n",
    "        # at the layer\n",
    "        layer_copy = self.layer.copy()        \n",
    "            \n",
    "        bias, weight,neurons = self.biases, self.weights, self.neuron_value\n",
    "        for k in layer_copy:\n",
    "            neuron_string = 'neuron'+str(k)\n",
    "            weight_string = 'weight'+ str(k)\n",
    "            bias_string = 'bias'+str(k)\n",
    "            \n",
    "            #update each neuron value from curent weight and bias\n",
    "            for enum,i in enumerate(neurons[neuron_string]):\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                serious correction is needed here in updating neuron values\n",
    "                \"\"\"\n",
    "                out = [neurons[neuron_string][enum]+=self.cost_function(l,\n",
    "                                               weight[weight_string][enum],\n",
    "                                               bias[bias_string][enum]) for l in x_val]\n",
    "            x_val = self.neuron_value[neuron_string]\n",
    "        \n",
    "        outp = neurons['neuron'+str(layer_copy[-1])]\n",
    "        return outp\n",
    "            \n",
    "    \n",
    "    def compute(self, data):\n",
    "        output = [self.bit_compute(l) for l in data]\n",
    "        return output\n",
    "    \n",
    "    def evaluate(self, data, y, evaluation_type = ['accuracy','f1_score','precision']):\n",
    "        value = self.compute(data)\n",
    "        evaluation = [int(np.round(i))==int(j) for i,j in zip(value,y)]\n",
    "        \n",
    "        if evaluation_type == ['accuracy']:\n",
    "            out = sum(evaluation)/len(evaluation)\n",
    "        elif 'precision' in evaluation_type:\n",
    "            # precision\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def plot_training(self):\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        #self.eval_fit\n",
    "        x = self.eval_fit.keys()\n",
    "        y = self.eval_fit.values\n",
    "        \n",
    "        sns.lineplot(x,y)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, model = 'All'):\n",
    "        \"\"\"\n",
    "        model can be: bias, weights, neuron_values\n",
    "        save biases\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        # write a file\n",
    "        f = open(\"model.pkl\", \"w\")\n",
    "        #dump biases\n",
    "        pickle.dump(self.biases, f)\n",
    "        #dump weights\n",
    "        pickle.dump(self.weights,f)\n",
    "        #dump neuron values\n",
    "        pickle.dump(self.neuron_value, f)\n",
    "        f.close()\n",
    "        return\n",
    "    \n",
    "    def load_model(self,model_dir =''):\n",
    "        f = open(model_dir, \"r\")\n",
    "        biases = pickle.load(f)\n",
    "        weights = pickle.load(f)\n",
    "        neurons = pickle.load(f)\n",
    "        f.close()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, values):\n",
    "        #\n",
    "        return classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
